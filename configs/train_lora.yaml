model_name: meta-llama/Llama-3-8b-instruct
method: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
train:
  epochs: 2
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  learning_rate: 0.0002
  bf16: true
data:
  input_key: report_text
  target_key: schema_json
  format: jsonl
